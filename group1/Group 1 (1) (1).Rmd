---
title: "The Relationship between Crime Types in the City of Chicago."
output: pdf_document
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	fig.asp = 0.8,
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	out.width = "99%"
)
# document
# http://rmarkdown.rstudio.com

# Import required libraries
if (!require('knitr')) install.packages('knitr'); library('knitr')
if (!require('dplyr')) install.packages('dplyr'); library('dplyr') 
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if (!require('purrr')) install.packages('purrr'); library('purrr')
if (!require('tidyr')) install.packages('tidyr'); library('tidyr')
if (!require('reshape')) install.packages('reshape'); library('reshape')

#Read in required libraries
library(moderndive)
library(gapminder)
library(skimr)
library(kableExtra)
library(gridExtra)
library(tidyverse)
library(cluster)
library(ggdendro)
library(factoextra)
library(GGally)
library(tidyr)

#Set the seed
set.seed(125)

#Read in the data
df <- read.csv('crime_data.csv')
```

# Introduction

The City of Chicago contains 77 communities and data is available from the Chicago Police Department (CPD) from 2002 to 2015 on all reported incidents of crime at community level. This report will be looking at a subset of this data from 2002 and specifically the crimes of theft, narcotics, motor vehicle theft, battery and assault. These crimes will be used to investigate the question of interest: which communities are similar in terms of their levels of crime? Answering this question allows us to give more information to the Chicago Police Department for predicting what kind of crimes would be in a community by the rates of other crimes. Also, levels of crime have an impact on house prices. By clustering communities by similar crime rates we will be able to see what areas will be similar in terms of house prices. The question will be answered through clustering techniques, and the clustering techniques will be compared to see what one is most effective in answering the question. 


# Exploratory Analysis

Before any clustering could begin for the data set, exploratory analysis was conducted. Figure 1 shows a box plot illustrating the distribution of each data point, where each data point represents the ratio of number of committed crimes over the population of each community with the mean plotted as a blue dot. The variables theft, narcotics and battery all have outlying observations, illustrating that  certain communities have a significantly high count of these types of crime. To overcome this issue, one method of clustering that can be used is hierarchical clustering. This allow such data to be grouped and easily interpreted when the dendrogram plot is illustrated. 

Although hierarchical clustering will help us illustrate correlations more easily, it does not always provide the best solutions. Using K-means alongside hierarchical will allow us to compute tighter clusters between observations and accepts clusters of different shapes and sizes such as elliptical clusters. 


```{r, echo=FALSE}

#SUBSETTING THE DATA

# Fix column names
df = df %>% rename_all(function(x) gsub(" ", "_", x))
df = df %>% rename_all(function(x) gsub("\\.", "_", x))

# Select year (2002)
df = subset(df, Year==2002) 
# select theft, narcotics, motor vehicle theft, battery, assault
df = subset(df,select = c(Number_Community, Name_Community, Population, THEFT, NARCOTICS, MOTOR_VEHICLE_THEFT, BATTERY,
                                    ASSAULT,Year))
# Make a copy of original data set
#set them to be ratios of population * 10000 to get the number per 10000 people to keep consistency
df_ <- df
df_$THEFT <- round((df_$THEFT/df_$Population)*10000)
df_$NARCOTICS <- round((df_$NARCOTICS/df_$Population)*10000)
df_$MOTOR_VEHICLE_THEFT <- round((df_$MOTOR_VEHICLE_THEFT/df_$Population)*10000)
df_$BATTERY <- round((df_$BATTERY/df_$Population)*10000)
df_$ASSAULT <- round((df_$ASSAULT/df_$Population)*10000)

df.summary = subset(df,select = c(Name_Community, Population, THEFT, NARCOTICS, MOTOR_VEHICLE_THEFT, BATTERY,
                                  ASSAULT))

#Summary Statistics 
my_skim <- skim_with(numeric = sfl(hist = NULL))
```

```{r plot1, echo = FALSE, fig.cap="Boxplot showing the rate of selected crime committed over a number of communities. The blue dot illustrate the mean crime rate for each type of crime.", out.width="60%", fig.align = "center"}
# Melt the data
df = melt(df,id=c("Number_Community","Name_Community","Population","Year"))
df = dplyr::rename(df, count = value)
df$Ratio = df$count / df$Population

#Boxplot

#colnames(df)[which(names(df) == "variable")] <- "Crime"

df %>% 
  ggplot(aes(x=variable ,y=Ratio, fill=variable)) +
  geom_boxplot(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +
  geom_jitter(width=0.1,alpha=0.2) +
  stat_summary(fun=mean, geom="point", size = 2, color = 5) +
  xlab("Crime Committed") +
  ylab("Ratio of the number of Crimes Committed over population")

```

From looking at Figure 2, all pairs of variables are plotted as scatterplots above and below the diagonal. We can see very strong correlations between the pairs Battery & Assault and moderately strong correlations between all the types of crime except Narcotics. This will help us understand that if a community had a high number of Battery then, that same community would likely contain high numbers of Assault. 

```{r pairs, echo=FALSE,out.width="60%", fig.cap="Pairs showing the correlation between the types of crime within the communities in Chicago "}
par(mar= c(1,1,1,1)) 
df.pairs = subset(df_, select=c(Population, THEFT, NARCOTICS, MOTOR_VEHICLE_THEFT, BATTERY, ASSAULT))
colnames(df.pairs)[which(names(df.pairs) == "MOTOR.VEHICLE.THEFT")] <- "VEHICLE THEFT"
ggpairs(df.pairs)
```


```{r include=FALSE}


clust_dt <- df_ %>% dplyr::select(
  "ASSAULT", "BATTERY", "MOTOR_VEHICLE_THEFT", "NARCOTICS", "THEFT"
)

row.names(clust_dt) <- df_$Name_Community

clust_dt.scale <-scale(clust_dt)

clust_dist <- dist(clust_dt.scale)
```


# Formal Analysis

Clustering is an unsupervised machine learning technique used to partition observations into subgroups or clusters. In theory, we want observations in the same cluster to be similar (i.e. have a high intra-class similarity) while observations in different clusters to be as dissimilar as possible (i.e. have a low inter-class similarity). The methods of clustering that will be used to answer the question of interest are Hierarchical clustering and K Means clustering.

## Hierarchical Clustering
Hierarchical clustering creates a hierarchical nested clustering tree by calculating the similarity between data points of different categories. In a cluster tree, the original data points of different categories are the lowest level of the tree, and the top level of the tree is the root node of a cluster. There are two ways to create cluster trees. Divisive clustering and agglomerative clustering. Agglomerative clustering is used for this analysis with complete linkage. This begins with every case being a cluster by itself. At each step, similar clusters are merged until all observations are in one cluster. 

Hierarchical clustering can be visualised through a dendrogram which is a visualisation of a distance matrix. The main use of a dendrogram is to work out the best way to allocate objects to clusters. Figure... plots each cluster and distance. Looking at this figure, it is clear that there may be three clusters in the data which are coloured to represent each cluster. It is easy to see where the first cluster (red), the second cluster (green), and the third cluster (blue) begin.

```{r , echo=FALSE, out.width="60%",fig.align = "center"}
clust <- hclust(clust_dist,method = "complete") 
fviz_dend(as.dendrogram(clust), rect = TRUE, k = 3)
```


## K-Means

The goal of K-means is to find k groups in the data. K-means attempts to find the assignment of observations to a fixed number of clusters K, that minimises the sum over all clusters of the sum of squares within clusters.
In k-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to the mean of points assigned to the cluster. New centroids are then computed as the average of all observations in a cluster and then each observation is assigned to its closest centroid, and this is repeated until the observations are not reassigned or the maximum number of iterations is reached.  Since there is no a response variable, the objective here is to try to find the number of clusters that minimises the total distance.

Since there was no prior knowledge on how many clusters would be our optimal numbers, an elbow plot was used , shown in Figure... to decide on the number 'k' of clusters to be used. The main elbow occurred at 4 suggesting k = 4 clusters would be suitable.

```{r , echo=FALSE, out.width="60%", fig.align = "center"}
fviz_nbclust(clust_dt, kmeans, method = "wss")
```

A plot of the cluster results was constructed to assess the choice of the number of clusters. Figure ... shows a visualization of the k-means clustering. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="60%", fig.align = "center"}
k4 <- kmeans(clust_dt, centers = 4, nstart = 25)
fviz_cluster(k4, geom = "point",  data = clust_dt, ellipse.type = "norm", 
            ggtheme = theme_minimal(), main="k = 4")
```

# Discussion of Results

Silhouette plots were created to compare the two different methods of clustering, Hierarchical and K-means. The silhouette value measures how similar an object is to its own cluster by comparing to the other clusters. It ranges from -1 to 1, where a high value indicates an object has matched well to its own cluster and not to other clusters and thus means a "good" clustering fit.
The width of the silhouette is defined as:

$$s_i=\frac{(b_i-a_i)}{\max\{a_i,b_i\}}$$

where $a_i$ is the average distance between observation $i$ and the other observations in $i$'s cluster,
$b_i$ is the minimum average distance between observation $i$ and the observations in other clusters.


```{r sil1, echo=FALSE, fig.cap="Silhouette Plot for Hierarchical and K-Means Clustering Techniques",out.width="50%",fig.width=3, fig.height=3,fig.show="hold",fig.align="centre"}
par(mar=c(4,4,.1,.1))
hccut <- hcut(clust_dt.scale,k=3,hc_method="complete") #does the same as above but just keeps the k=3 there
fviz_silhouette(hccut, print.summary = FALSE)

sil.km <- silhouette(k4$cluster, dist(df_[,c(4,5,6,7,8)]))
fviz_silhouette(sil.km, print.summary = FALSE)

```


From these figures and the average silhouette width, it can be said that for this dataset, the K-Means Clustering method is favorable over the Hierarchical Clustering method. The average width for Hierarchical was 0.51 and the K-Means value was 0.57. The difference in width isn't huge and so either could be said to be a good technique, however the one that will be used for further analysis will be the K-Means technique.

```{r result, echo=FALSE}
k4new <- df_
k4new$cluster <- k4$cluster
clust1 <- subset(k4new, k4new$cluster==1)
clust2 <- subset(k4new, k4new$cluster==2)
clust3 <- subset(k4new, k4new$cluster==3)
clust4 <- subset(k4new, k4new$cluster==4)
tablevalues1 <- c(mean(clust1$Population),mean(clust1$THEFT),mean(clust1$NARCOTICS),mean(clust1$MOTOR_VEHICLE_THEFT),mean(clust1$BATTERY),mean(clust1$ASSAULT))
tablevalues2 <- c(mean(clust2$Population),mean(clust2$THEFT),mean(clust2$NARCOTICS),mean(clust2$MOTOR_VEHICLE_THEFT),mean(clust2$BATTERY),mean(clust2$ASSAULT))
tablevalues3 <- c(mean(clust3$Population),mean(clust3$THEFT),mean(clust3$NARCOTICS),mean(clust3$MOTOR_VEHICLE_THEFT),mean(clust3$BATTERY),mean(clust3$ASSAULT))
tablevalues4 <- c(mean(clust4$Population),mean(clust4$THEFT),mean(clust4$NARCOTICS),mean(clust4$MOTOR_VEHICLE_THEFT),mean(clust4$BATTERY),mean(clust4$ASSAULT))
tablev <- data.frame(tablevalues1,tablevalues2,tablevalues3,tablevalues4)
rownames(tablev) <- c("Population Average", "Average Theft count", "Average Narcotics count","Average Vehicle Theft Count","Average Battery Count", "Average Assault Count")
colnames(tablev) <- c("Cluster 1","Cluster 2","Cluster 3","Cluster 4")
knitr::kable(tablev,caption = "Average Crime Numbers for each cluster")

```

Using the K-Means clustering method, a results table above shows the average count data for each of the clusters as well as the average population.